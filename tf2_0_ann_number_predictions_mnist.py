# -*- coding: utf-8 -*-
"""TF2.0 ANN Number Predictions MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Il5Ct4p-cbkQdUix2OWfKvxSZJUn1pRP
"""

!pip install tensorflow

import tensorflow as tf
print(tf.__version__)

# Load in the data of the number recognition values 0 to 9 in grayscale
mnist = tf.keras.datasets.mnist 

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# we need to normalize the training and test pixel values with 255 because that
# gives the full range of colors
x_train, x_test = x_train/255.0, x_test/255.0
# the result will be the width and height and N samples
print("x_train_shape:", x_train.shape)

# build the model
model = tf.keras.models.Sequential([
                                    # the input is 28 because that's the result of
                                    # dividing the training and test set by 255
                                    tf.keras.layers.Flatten(input_shape=(28,28)),
                                    # we will have 128 layers of neurons
                                    tf.keras.layers.Dense(128, activation='relu'),
                                    # drop out neurons, so we don't rely only 1 one of them
                                    tf.keras.layers.Dropout(0.2),
                                    # we will have 10 classes classification, so we
                                    tf.keras.layers.Dense(10, activation='softmax')
])

# compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)

# plot loss per iteration
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='loss')
plt.plot(r.history['val_loss'], label='val_loss')
plt.legend()

# plot accuracy per iteration
plt.plot(r.history['accuracy'], label='acc')
plt.plot(r.history['val_accuracy'], label='val_acc')
plt.legend()

# evaluate the model
# loss and accuracy
print(model.evaluate(x_test, y_test))

# Confusion matrix
# For each label, how many predictions correspond to a label

from sklearn.metrics import confusion_matrix
import numpy as np
import itertools

def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',
cmap = plt.cm.Blues):
    if normalize:
      cm = cm.astype('float')/cm.sum(axis=1)[:, np.newaxis]
      print("Normalized confusion matrix")
    else:
      print("Confusion matrix, without normalization")
    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    threshold = cm.max()/2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment='center',
               color='white' if cm[i,j]>threshold else 'black')
      
    plt.tight_layout()
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.show()
     
p_test = model.predict(x_test).argmax(axis=1)
cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# show some misclassified examples
misclassified_idx = np.where(p_test !=y_test)[0]
i = np.random.choice(misclassified_idx)
plt.imshow(x_test[i], cmap='gray')
#plt.title('True label: %s' % y_test[i]);
plt.title('True label: %s Predicted label: %s' % (y_test[i],p_test[i]));

!pip freeze > requirements_ANN_Number_predictions.txt

!l

!pwd

from google.colab import files
files.download("/content/requirements_ANN_Number_predictions.txt")

